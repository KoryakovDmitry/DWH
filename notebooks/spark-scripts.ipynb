{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95d8403-88e9-432f-9370-9c5463578c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, expr, when, avg\n",
    "from pyspark.sql.types import TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889a2dfb-43a3-4979-b121-642cb145c690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5cb2e0982c40:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JupyterSparkCluster</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xf8098c2d6c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Инициализируем Spark Session с добавлением PostgreSQL JDBC драйвера\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"JupyterSparkCluster\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.5.4\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43590efa-56e6-4a71-b617-7a25619425e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры подключения к PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/postgres\"\n",
    "connection_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7cee6-251a-4427-bff3-7141fc7cd1bd",
   "metadata": {},
   "source": [
    "# Заполнение таблиц измерений и фактов в DWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5149f11b-133b-46d7-a652-fc6212034764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(spark, jdbc_url, connection_properties, source_table, target_table, selected_columns, mode='append'):\n",
    "    \"\"\"\n",
    "    Загружает данные из исходной таблицы в целевую таблицу DWH с выборкой необходимых колонок.\n",
    "\n",
    "    :param spark: Объект SparkSession.\n",
    "    :param jdbc_url: JDBC URL для подключения к PostgreSQL.\n",
    "    :param connection_properties: Словарь с параметрами подключения (user, password, driver).\n",
    "    :param source_table: Имя исходной таблицы (схема.таблица).\n",
    "    :param target_table: Имя целевой таблицы в DWH (схема.таблица).\n",
    "    :param selected_columns: Список колонок для выборки из исходной таблицы.\n",
    "    :param mode: Режим записи ('overwrite' или 'append'). По умолчанию 'append'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Чтение данных из исходной таблицы\n",
    "    df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=source_table,\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    # Выбор необходимых колонок\n",
    "    df_selected = df.select(*selected_columns)\n",
    "    \n",
    "    # Добавление колонки с временем загрузки\n",
    "    df_selected = df_selected.withColumn(\"load_dttm\", current_timestamp())\n",
    "    \n",
    "    # Запись данных в целевую таблицу\n",
    "    df_selected.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=target_table,\n",
    "        mode=mode,\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    print(f\"Таблица {target_table} успешно загружена.\")\n",
    "    return df_selected\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2478ab1f-65ae-4880-acef-5751a3ffd087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.d_craftsmans успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки dwh.d_craftsmans\n",
    "source_table_craftsmans = \"source3.craft_market_craftsmans\"\n",
    "target_table_craftsmans = \"dwh.d_craftsmans\"\n",
    "selected_columns_craftsmans = [\n",
    "    # \"craftsman_id\",\n",
    "    \"craftsman_name\",\n",
    "    \"craftsman_address\",\n",
    "    \"craftsman_birthday\",\n",
    "    \"craftsman_email\"\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.d_craftsmans\n",
    "dwh_d_craftsmans = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_craftsmans,\n",
    "    target_table=target_table_craftsmans,\n",
    "    selected_columns=selected_columns_craftsmans,\n",
    "    mode='append'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aed554e-28fa-4a7c-a68e-dcf83c9df713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.d_customers успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки dwh.d_customers\n",
    "source_table_customers = \"source3.craft_market_customers\"\n",
    "target_table_customers = \"dwh.d_customers\"\n",
    "selected_columns_customers = [\n",
    "    # \"customer_id\",\n",
    "    \"customer_name\",\n",
    "    \"customer_address\",\n",
    "    \"customer_birthday\",\n",
    "    \"customer_email\"\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.d_customers\n",
    "dwh_d_customers = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_customers,\n",
    "    target_table=target_table_customers,\n",
    "    selected_columns=selected_columns_customers,\n",
    "    mode='append'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d4504ac-b0c9-4c6a-9f26-f420cc1cc692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer_name='Anna-maria Lamba', customer_address='8 Prairieview Alley', customer_birthday=datetime.date(1990, 7, 27), customer_email='alamba1@ted.com', load_dttm=datetime.datetime(2024, 12, 26, 4, 57, 36, 827196)),\n",
       " Row(customer_name='Kim Coonihan', customer_address='3 Hanson Center', customer_birthday=datetime.date(1992, 5, 26), customer_email='kcoonihan2@angelfire.com', load_dttm=datetime.datetime(2024, 12, 26, 4, 57, 36, 827196)),\n",
       " Row(customer_name='Lanny Vasse', customer_address='9 Westridge Alley', customer_birthday=datetime.date(2003, 6, 12), customer_email='lvasse3@pbs.org', load_dttm=datetime.datetime(2024, 12, 26, 4, 57, 36, 827196)),\n",
       " Row(customer_name='Alice Treneman', customer_address='68462 Meadow Valley Drive', customer_birthday=datetime.date(1995, 7, 26), customer_email='atreneman4@cmu.edu', load_dttm=datetime.datetime(2024, 12, 26, 4, 57, 36, 827196)),\n",
       " Row(customer_name='Flo Morant', customer_address='2453 Crescent Oaks Avenue', customer_birthday=datetime.date(1997, 10, 31), customer_email='fmorant5@shinystat.com', load_dttm=datetime.datetime(2024, 12, 26, 4, 57, 36, 827196))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwh_d_customers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82eca057-6aef-4ce6-9504-f1e4eaa99f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.d_products успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки dwh.d_products\n",
    "source_table_products = \"source2.craft_market_masters_products\"  # Предполагается, что products находятся здесь\n",
    "target_table_products = \"dwh.d_products\"\n",
    "selected_columns_products = [\n",
    "    # \"product_id\",\n",
    "    \"product_name\",\n",
    "    \"product_description\",\n",
    "    \"product_type\",\n",
    "    \"product_price\"\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.d_products\n",
    "dwh_d_products = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_products,\n",
    "    target_table=target_table_products,\n",
    "    selected_columns=selected_columns_products,\n",
    "    mode='append'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf67021-4cc6-49ab-9a19-15e366330bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.f_orders успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "source_table_orders = \"source1.craft_market_wide\"\n",
    "target_table_orders = \"dwh.f_orders\"\n",
    "selected_columns_orders = [\n",
    "    \"order_created_date\",\n",
    "    \"order_completion_date\",\n",
    "    \"order_status\",\n",
    "    \"customer_id\",\n",
    "    \"craftsman_id\",\n",
    "    \"product_id\",\n",
    "    \"order_id\",\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.f_orders\n",
    "dwh_f_orders = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_orders,\n",
    "    target_table=target_table_orders,\n",
    "    selected_columns=selected_columns_orders,\n",
    "    mode='append'  # Используем 'append' для добавления данных\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e32350-20b1-4649-9dca-3aaad2a1c85d",
   "metadata": {},
   "source": [
    "# Инкрементальная Загрузка Витрины Данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78412350-ca78-41ee-9ecd-f354ffecde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, expr, count, floor\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def load_datamart_incremental(spark, jdbc_url, connection_properties, source_table, load_dates_table, target_datamart_table, report_period):\n",
    "    \"\"\"\n",
    "    Инкрементально загружает данные в витрину данных dwh.craftsman_report_datamart.\n",
    "    \n",
    "    :param spark: SparkSession\n",
    "    :param jdbc_url: JDBC URL для подключения к PostgreSQL.\n",
    "    :param connection_properties: Словарь с параметрами подключения (user, password, driver).\n",
    "    :param source_table: Таблица фактов (dwh.f_orders).\n",
    "    :param load_dates_table: Таблица для отслеживания дат загрузки (dwh.load_dates_craftsman_report_datamart).\n",
    "    :param target_datamart_table: Витрина данных (dwh.craftsman_report_datamart).\n",
    "    :param report_period: Отчетный период в формате 'YYYY-MM'.\n",
    "    \"\"\"\n",
    "    # Шаг 1: Получаем последнюю дату загрузки\n",
    "    last_load_date_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=load_dates_table,\n",
    "        properties=connection_properties\n",
    "    ).orderBy(col(\"load_dttm\").desc()).limit(1)\n",
    "    \n",
    "    if last_load_date_df.count() > 0:\n",
    "        last_load_date = last_load_date_df.collect()[0]['load_dttm']\n",
    "        print(f\"Последняя дата загрузки: {last_load_date}\")\n",
    "    else:\n",
    "        last_load_date = lit('1970-01-01').cast(TimestampType())  # Начальное значение\n",
    "        print(\"Таблица load_dates пуста. Загружаем все данные.\")\n",
    "    \n",
    "    # Шаг 2: Извлекаем новые или обновлённые записи из таблицы фактов\n",
    "    df_new_orders = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=source_table,\n",
    "        properties=connection_properties\n",
    "    ).filter(col(\"load_dttm\") > last_load_date)\n",
    "    \n",
    "    if df_new_orders.count() == 0:\n",
    "        print(\"Нет новых данных для загрузки.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Загружаем {df_new_orders.count()} новых записей из таблицы фактов.\")\n",
    "    \n",
    "    # Шаг 3: Объединяем данные с таблицами измерений\n",
    "    df_joined = df_new_orders \\\n",
    "        .join(\n",
    "            spark.read.jdbc(url=jdbc_url, table=\"dwh.d_craftsmans\", properties=connection_properties),\n",
    "            on=\"craftsman_id\",\n",
    "            how=\"left\"\n",
    "        ) \\\n",
    "        .join(\n",
    "            spark.read.jdbc(url=jdbc_url, table=\"dwh.d_customers\", properties=connection_properties),\n",
    "            on=\"customer_id\",\n",
    "            how=\"left\"\n",
    "        ) \\\n",
    "        .join(\n",
    "            spark.read.jdbc(url=jdbc_url, table=\"dwh.d_products\", properties=connection_properties),\n",
    "            on=\"product_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    # Шаг 4: Выполняем агрегации для расчёта метрик, исключая top_product_category\n",
    "    df_aggregated = df_joined.groupBy(\n",
    "        \"craftsman_id\", \n",
    "        \"craftsman_name\", \n",
    "        \"craftsman_address\", \n",
    "        \"craftsman_birthday\", \n",
    "        \"craftsman_email\"\n",
    "    ).agg(\n",
    "        (F.sum(\"product_price\") * 0.9).alias(\"craftsman_money\"),  # 10% платформа\n",
    "        (F.sum(\"product_price\") * 0.1).alias(\"platform_money\"),\n",
    "        F.count(\"order_id\").alias(\"count_order\"),\n",
    "        F.avg(\"product_price\").alias(\"avg_price_order\"),\n",
    "        F.avg(F.floor(F.months_between(F.current_date(), F.col(\"customer_birthday\")) / 12)).alias(\"avg_age_customer\"),\n",
    "        F.expr(\"percentile_approx(datediff(order_completion_date, order_created_date), 0.5)\").alias(\"median_time_order_completed\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"created\", 1)).alias(\"count_order_created\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"in progress\", 1)).alias(\"count_order_in_progress\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"delivery\", 1)).alias(\"count_order_delivery\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"done\", 1)).alias(\"count_order_done\"),\n",
    "        F.count(F.when(~col(\"order_status\").isin(\"done\"), 1)).alias(\"count_order_not_done\")\n",
    "    )\n",
    "    \n",
    "    # Шаг 5: Вычисляем top_product_category отдельно\n",
    "    # Считаем количество каждого product_type для каждого craftsman_id\n",
    "    df_product_counts = df_joined.groupBy(\"craftsman_id\", \"product_type\") \\\n",
    "        .agg(F.count(\"*\").alias(\"product_type_count\"))\n",
    "    \n",
    "    # Определяем самое частое product_type для каждого craftsman_id\n",
    "    window_spec = Window.partitionBy(\"craftsman_id\").orderBy(F.desc(\"product_type_count\"))\n",
    "    \n",
    "    df_top_product = df_product_counts.withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "        .filter(F.col(\"rank\") == 1) \\\n",
    "        .select(\"craftsman_id\", F.col(\"product_type\").alias(\"top_product_category\"))\n",
    "    \n",
    "    # Шаг 6: Объединяем агрегированные данные с top_product_category\n",
    "    df_final = df_aggregated.join(df_top_product, on=\"craftsman_id\", how=\"left\") \\\n",
    "        .withColumn(\"report_period\", lit(report_period)) \\\n",
    "        # .withColumn(\"load_dttm\", F.current_timestamp())\n",
    "    \n",
    "    # Шаг 7: Записываем агрегированные данные в витрину данных\n",
    "    df_final.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=target_datamart_table,\n",
    "        mode='append',\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    print(f\"Витрина данных {target_datamart_table} успешно обновлена.\")\n",
    "    \n",
    "    # Шаг 8: Обновляем таблицу дат загрузки\n",
    "    df_load_date = spark.range(1).withColumn(\"load_dttm\", current_timestamp()).select(\"load_dttm\")\n",
    "    df_load_date.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=load_dates_table,\n",
    "        mode='append',\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    print(f\"Таблица {load_dates_table} обновлена.\")\n",
    "    return df_final, df_load_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abf42530-72fb-4a2c-b7b1-014dde6352c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица load_dates пуста. Загружаем все данные.\n",
      "Загружаем 999 новых записей из таблицы фактов.\n",
      "Витрина данных dwh.craftsman_report_datamart успешно обновлена.\n",
      "Таблица dwh.load_dates_craftsman_report_datamart обновлена.\n"
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки витрины данных\n",
    "source_table_for_datamart = \"dwh.f_orders\"  # Источник данных для витрины\n",
    "load_dates_table = \"dwh.load_dates_craftsman_report_datamart\"\n",
    "target_datamart_table = \"dwh.craftsman_report_datamart\"\n",
    "report_period = \"2021-03\"  # Формат: 'ГГГГ-ММ'\n",
    "\n",
    "# Вызов функции для инкрементальной загрузки витрины данных\n",
    "dwh_load_dates_craftsman_report_datamart, dwh_craftsman_report_datamart = load_datamart_incremental(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_for_datamart,\n",
    "    load_dates_table=load_dates_table,\n",
    "    target_datamart_table=target_datamart_table,\n",
    "    report_period=report_period\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b98e2d-a366-4f46-abcb-8e9b56873975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
