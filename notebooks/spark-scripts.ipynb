{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95d8403-88e9-432f-9370-9c5463578c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, expr, when, avg\n",
    "from pyspark.sql.types import TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889a2dfb-43a3-4979-b121-642cb145c690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b49692f8233c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JupyterSparkCluster</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xe3db00de6150>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Инициализируем Spark Session с добавлением PostgreSQL JDBC драйвера\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"JupyterSparkCluster\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.5.4\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43590efa-56e6-4a71-b617-7a25619425e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры подключения к PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/postgres\"\n",
    "connection_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7cee6-251a-4427-bff3-7141fc7cd1bd",
   "metadata": {},
   "source": [
    "# Заполнение таблиц измерений и фактов в DWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5149f11b-133b-46d7-a652-fc6212034764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(spark, jdbc_url, connection_properties, source_table, target_table, selected_columns, mode='append'):\n",
    "    \"\"\"\n",
    "    Загружает данные из исходной таблицы в целевую таблицу DWH с выборкой необходимых колонок.\n",
    "\n",
    "    :param spark: Объект SparkSession.\n",
    "    :param jdbc_url: JDBC URL для подключения к PostgreSQL.\n",
    "    :param connection_properties: Словарь с параметрами подключения (user, password, driver).\n",
    "    :param source_table: Имя исходной таблицы (схема.таблица).\n",
    "    :param target_table: Имя целевой таблицы в DWH (схема.таблица).\n",
    "    :param selected_columns: Список колонок для выборки из исходной таблицы.\n",
    "    :param mode: Режим записи ('overwrite' или 'append'). По умолчанию 'append'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Чтение данных из исходной таблицы\n",
    "    df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=source_table,\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    # Выбор необходимых колонок\n",
    "    df_selected = df.select(*selected_columns)\n",
    "    \n",
    "    # Добавление колонки с временем загрузки\n",
    "    df_selected = df_selected.withColumn(\"load_dttm\", current_timestamp())\n",
    "    \n",
    "    # Запись данных в целевую таблицу\n",
    "    df_selected.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=target_table,\n",
    "        mode=mode,\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    print(f\"Таблица {target_table} успешно загружена.\")\n",
    "    return df_selected\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2478ab1f-65ae-4880-acef-5751a3ffd087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.d_craftsmans успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки dwh.d_craftsmans\n",
    "source_table_craftsmans = \"source3.craft_market_craftsmans\"\n",
    "target_table_craftsmans = \"dwh.d_craftsmans\"\n",
    "selected_columns_craftsmans = [\n",
    "    # \"craftsman_id\",\n",
    "    \"craftsman_name\",\n",
    "    \"craftsman_address\",\n",
    "    \"craftsman_birthday\",\n",
    "    \"craftsman_email\"\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.d_craftsmans\n",
    "dwh_d_craftsmans = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_craftsmans,\n",
    "    target_table=target_table_craftsmans,\n",
    "    selected_columns=selected_columns_craftsmans,\n",
    "    mode='append'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aed554e-28fa-4a7c-a68e-dcf83c9df713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.d_customers успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки dwh.d_customers\n",
    "source_table_customers = \"source3.craft_market_customers\"\n",
    "target_table_customers = \"dwh.d_customers\"\n",
    "selected_columns_customers = [\n",
    "    # \"customer_id\",\n",
    "    \"customer_name\",\n",
    "    \"customer_address\",\n",
    "    \"customer_birthday\",\n",
    "    \"customer_email\"\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.d_customers\n",
    "dwh_d_customers = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_customers,\n",
    "    target_table=target_table_customers,\n",
    "    selected_columns=selected_columns_customers,\n",
    "    mode='append'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d4504ac-b0c9-4c6a-9f26-f420cc1cc692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer_name='Anna-maria Lamba', customer_address='8 Prairieview Alley', customer_birthday=datetime.date(1990, 7, 27), customer_email='alamba1@ted.com', load_dttm=datetime.datetime(2024, 12, 26, 4, 5, 13, 700602)),\n",
       " Row(customer_name='Kim Coonihan', customer_address='3 Hanson Center', customer_birthday=datetime.date(1992, 5, 26), customer_email='kcoonihan2@angelfire.com', load_dttm=datetime.datetime(2024, 12, 26, 4, 5, 13, 700602)),\n",
       " Row(customer_name='Lanny Vasse', customer_address='9 Westridge Alley', customer_birthday=datetime.date(2003, 6, 12), customer_email='lvasse3@pbs.org', load_dttm=datetime.datetime(2024, 12, 26, 4, 5, 13, 700602)),\n",
       " Row(customer_name='Alice Treneman', customer_address='68462 Meadow Valley Drive', customer_birthday=datetime.date(1995, 7, 26), customer_email='atreneman4@cmu.edu', load_dttm=datetime.datetime(2024, 12, 26, 4, 5, 13, 700602)),\n",
       " Row(customer_name='Flo Morant', customer_address='2453 Crescent Oaks Avenue', customer_birthday=datetime.date(1997, 10, 31), customer_email='fmorant5@shinystat.com', load_dttm=datetime.datetime(2024, 12, 26, 4, 5, 13, 700602))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwh_d_customers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82eca057-6aef-4ce6-9504-f1e4eaa99f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.d_products успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки dwh.d_products\n",
    "source_table_products = \"source2.craft_market_masters_products\"  # Предполагается, что products находятся здесь\n",
    "target_table_products = \"dwh.d_products\"\n",
    "selected_columns_products = [\n",
    "    # \"product_id\",\n",
    "    \"product_name\",\n",
    "    \"product_description\",\n",
    "    \"product_type\",\n",
    "    \"product_price\"\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.d_products\n",
    "dwh_d_products = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_products,\n",
    "    target_table=target_table_products,\n",
    "    selected_columns=selected_columns_products,\n",
    "    mode='append'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf67021-4cc6-49ab-9a19-15e366330bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.f_orders успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "source_table_orders = \"source1.craft_market_wide\"\n",
    "target_table_orders = \"dwh.f_orders\"\n",
    "selected_columns_orders = [\n",
    "    \"order_created_date\",\n",
    "    \"order_completion_date\",\n",
    "    \"order_status\",\n",
    "    \"customer_id\",\n",
    "    \"craftsman_id\",\n",
    "    \"product_id\",\n",
    "    \"order_id\",\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.f_orders\n",
    "dwh_f_orders = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_orders,\n",
    "    target_table=target_table_orders,\n",
    "    selected_columns=selected_columns_orders,\n",
    "    mode='append'  # Используем 'append' для добавления данных\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e32350-20b1-4649-9dca-3aaad2a1c85d",
   "metadata": {},
   "source": [
    "# Инкрементальная Загрузка Витрины Данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78412350-ca78-41ee-9ecd-f354ffecde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, expr, count, floor\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def load_datamart_incremental(spark, jdbc_url, connection_properties, source_table, load_dates_table, target_datamart_table, report_period):\n",
    "    \"\"\"\n",
    "    Инкрементально загружает данные в витрину данных dwh.craftsman_report_datamart.\n",
    "    \n",
    "    :param spark: SparkSession\n",
    "    :param jdbc_url: JDBC URL для подключения к PostgreSQL.\n",
    "    :param connection_properties: Словарь с параметрами подключения (user, password, driver).\n",
    "    :param source_table: Таблица фактов (dwh.f_orders).\n",
    "    :param load_dates_table: Таблица для отслеживания дат загрузки (dwh.load_dates_craftsman_report_datamart).\n",
    "    :param target_datamart_table: Витрина данных (dwh.craftsman_report_datamart).\n",
    "    :param report_period: Отчетный период в формате 'YYYY-MM'.\n",
    "    \"\"\"\n",
    "    # Шаг 1: Получаем последнюю дату загрузки\n",
    "    last_load_date_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=load_dates_table,\n",
    "        properties=connection_properties\n",
    "    ).orderBy(col(\"load_dttm\").desc()).limit(1)\n",
    "    \n",
    "    if last_load_date_df.count() > 0:\n",
    "        last_load_date = last_load_date_df.collect()[0]['load_dttm']\n",
    "        print(f\"Последняя дата загрузки: {last_load_date}\")\n",
    "    else:\n",
    "        last_load_date = lit('1970-01-01').cast(TimestampType())  # Начальное значение\n",
    "        print(\"Таблица load_dates пуста. Загружаем все данные.\")\n",
    "    \n",
    "    # Шаг 2: Извлекаем новые или обновлённые записи из таблицы фактов\n",
    "    df_new_orders = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=source_table,\n",
    "        properties=connection_properties\n",
    "    ).filter(col(\"load_dttm\") > last_load_date)\n",
    "    \n",
    "    if df_new_orders.count() == 0:\n",
    "        print(\"Нет новых данных для загрузки.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Загружаем {df_new_orders.count()} новых записей из таблицы фактов.\")\n",
    "    \n",
    "    # Шаг 3: Объединяем данные с таблицами измерений\n",
    "    df_joined = df_new_orders \\\n",
    "        .join(\n",
    "            spark.read.jdbc(url=jdbc_url, table=\"dwh.d_craftsmans\", properties=connection_properties),\n",
    "            on=\"craftsman_id\",\n",
    "            how=\"left\"\n",
    "        ) \\\n",
    "        .join(\n",
    "            spark.read.jdbc(url=jdbc_url, table=\"dwh.d_customers\", properties=connection_properties),\n",
    "            on=\"customer_id\",\n",
    "            how=\"left\"\n",
    "        ) \\\n",
    "        .join(\n",
    "            spark.read.jdbc(url=jdbc_url, table=\"dwh.d_products\", properties=connection_properties),\n",
    "            on=\"product_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    # Шаг 4: Выполняем агрегации для расчёта метрик, исключая top_product_category\n",
    "    df_aggregated = df_joined.groupBy(\n",
    "        \"craftsman_id\", \n",
    "        \"craftsman_name\", \n",
    "        \"craftsman_address\", \n",
    "        \"craftsman_birthday\", \n",
    "        \"craftsman_email\"\n",
    "    ).agg(\n",
    "        (F.sum(\"product_price\") * 0.9).alias(\"craftsman_money\"),  # 10% платформа\n",
    "        (F.sum(\"product_price\") * 0.1).alias(\"platform_money\"),\n",
    "        F.count(\"order_id\").alias(\"count_order\"),\n",
    "        F.avg(\"product_price\").alias(\"avg_price_order\"),\n",
    "        F.avg(F.floor(F.months_between(F.current_date(), F.col(\"customer_birthday\")) / 12)).alias(\"avg_age_customer\"),\n",
    "        F.expr(\"percentile_approx(datediff(order_completion_date, order_created_date), 0.5)\").alias(\"median_time_order_completed\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"created\", 1)).alias(\"count_order_created\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"in progress\", 1)).alias(\"count_order_in_progress\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"delivery\", 1)).alias(\"count_order_delivery\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"done\", 1)).alias(\"count_order_done\"),\n",
    "        F.count(F.when(~col(\"order_status\").isin(\"done\"), 1)).alias(\"count_order_not_done\")\n",
    "    )\n",
    "    \n",
    "    # Шаг 5: Вычисляем top_product_category отдельно\n",
    "    # Считаем количество каждого product_type для каждого craftsman_id\n",
    "    df_product_counts = df_joined.groupBy(\"craftsman_id\", \"product_type\") \\\n",
    "        .agg(F.count(\"*\").alias(\"product_type_count\"))\n",
    "    \n",
    "    # Определяем самое частое product_type для каждого craftsman_id\n",
    "    window_spec = Window.partitionBy(\"craftsman_id\").orderBy(F.desc(\"product_type_count\"))\n",
    "    \n",
    "    df_top_product = df_product_counts.withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "        .filter(F.col(\"rank\") == 1) \\\n",
    "        .select(\"craftsman_id\", F.col(\"product_type\").alias(\"top_product_category\"))\n",
    "    \n",
    "    # Шаг 6: Объединяем агрегированные данные с top_product_category\n",
    "    df_final = df_aggregated.join(df_top_product, on=\"craftsman_id\", how=\"left\") \\\n",
    "        .withColumn(\"report_period\", lit(report_period)) \\\n",
    "        .withColumn(\"load_dttm\", F.current_timestamp())\n",
    "    \n",
    "    # Шаг 7: Записываем агрегированные данные в витрину данных\n",
    "    df_final.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=target_datamart_table,\n",
    "        mode='append',\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    print(f\"Витрина данных {target_datamart_table} успешно обновлена.\")\n",
    "    \n",
    "    # Шаг 8: Обновляем таблицу дат загрузки\n",
    "    current_load_dttm = F.current_timestamp()\n",
    "    df_load_date = spark.createDataFrame([(current_load_dttm,)], [\"load_dttm\"])\n",
    "    df_load_date.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=load_dates_table,\n",
    "        mode='append',\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    print(f\"Таблица {load_dates_table} обновлена.\")\n",
    "    return df_final, df_load_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abf42530-72fb-4a2c-b7b1-014dde6352c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица load_dates пуста. Загружаем все данные.\n",
      "Загружаем 999 новых записей из таблицы фактов.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Column load_dttm not found in schema Some(StructType(StructField(id,LongType,false),StructField(craftsman_id,LongType,false),StructField(craftsman_name,StringType,false),StructField(craftsman_address,StringType,false),StructField(craftsman_birthday,DateType,false),StructField(craftsman_email,StringType,false),StructField(craftsman_money,DecimalType(15,2),false),StructField(platform_money,LongType,false),StructField(count_order,LongType,false),StructField(avg_price_order,DecimalType(10,2),false),StructField(avg_age_customer,DecimalType(3,1),false),StructField(median_time_order_completed,DecimalType(10,1),true),StructField(top_product_category,StringType,false),StructField(count_order_created,LongType,false),StructField(count_order_in_progress,LongType,false),StructField(count_order_delivery,LongType,false),StructField(count_order_done,LongType,false),StructField(count_order_not_done,LongType,false),StructField(report_period,StringType,false))).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m report_period \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2021-03\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Формат: 'ГГГГ-ММ'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Вызов функции для инкрементальной загрузки витрины данных\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m dwh_load_dates_craftsman_report_datamart, dwh_craftsman_report_datamart \u001b[38;5;241m=\u001b[39m \u001b[43mload_datamart_incremental\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnection_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_table_for_datamart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_dates_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_dates_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_datamart_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_datamart_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 102\u001b[0m, in \u001b[0;36mload_datamart_incremental\u001b[0;34m(spark, jdbc_url, connection_properties, source_table, load_dates_table, target_datamart_table, report_period)\u001b[0m\n\u001b[1;32m     97\u001b[0m df_final \u001b[38;5;241m=\u001b[39m df_aggregated\u001b[38;5;241m.\u001b[39mjoin(df_top_product, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcraftsman_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_period\u001b[39m\u001b[38;5;124m\"\u001b[39m, lit(report_period)) \\\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_dttm\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mcurrent_timestamp())\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Шаг 7: Записываем агрегированные данные в витрину данных\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[43mdf_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_datamart_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_properties\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mВитрина данных \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_datamart_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m успешно обновлена.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Шаг 8: Обновляем таблицу дат загрузки\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column load_dttm not found in schema Some(StructType(StructField(id,LongType,false),StructField(craftsman_id,LongType,false),StructField(craftsman_name,StringType,false),StructField(craftsman_address,StringType,false),StructField(craftsman_birthday,DateType,false),StructField(craftsman_email,StringType,false),StructField(craftsman_money,DecimalType(15,2),false),StructField(platform_money,LongType,false),StructField(count_order,LongType,false),StructField(avg_price_order,DecimalType(10,2),false),StructField(avg_age_customer,DecimalType(3,1),false),StructField(median_time_order_completed,DecimalType(10,1),true),StructField(top_product_category,StringType,false),StructField(count_order_created,LongType,false),StructField(count_order_in_progress,LongType,false),StructField(count_order_delivery,LongType,false),StructField(count_order_done,LongType,false),StructField(count_order_not_done,LongType,false),StructField(report_period,StringType,false)))."
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки витрины данных\n",
    "source_table_for_datamart = \"dwh.f_orders\"  # Источник данных для витрины\n",
    "load_dates_table = \"dwh.load_dates_craftsman_report_datamart\"\n",
    "target_datamart_table = \"dwh.craftsman_report_datamart\"\n",
    "report_period = \"2021-03\"  # Формат: 'ГГГГ-ММ'\n",
    "\n",
    "# Вызов функции для инкрементальной загрузки витрины данных\n",
    "dwh_load_dates_craftsman_report_datamart, dwh_craftsman_report_datamart = load_datamart_incremental(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_for_datamart,\n",
    "    load_dates_table=load_dates_table,\n",
    "    target_datamart_table=target_datamart_table,\n",
    "    report_period=report_period\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe4506-4779-4fba-b439-94a3f157e047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
