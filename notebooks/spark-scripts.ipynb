{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95d8403-88e9-432f-9370-9c5463578c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, expr, when, avg\n",
    "from pyspark.sql.types import TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889a2dfb-43a3-4979-b121-642cb145c690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b49692f8233c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JupyterSparkCluster</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xe3db00de6150>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Инициализируем Spark Session с добавлением PostgreSQL JDBC драйвера\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"JupyterSparkCluster\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.5.4\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43590efa-56e6-4a71-b617-7a25619425e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры подключения к PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/postgres\"\n",
    "connection_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7cee6-251a-4427-bff3-7141fc7cd1bd",
   "metadata": {},
   "source": [
    "# Заполнение таблиц измерений и фактов в DWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5149f11b-133b-46d7-a652-fc6212034764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(spark, jdbc_url, connection_properties, source_table, target_table, selected_columns, mode='append'):\n",
    "    \"\"\"\n",
    "    Загружает данные из исходной таблицы в целевую таблицу DWH с выборкой необходимых колонок.\n",
    "\n",
    "    :param spark: Объект SparkSession.\n",
    "    :param jdbc_url: JDBC URL для подключения к PostgreSQL.\n",
    "    :param connection_properties: Словарь с параметрами подключения (user, password, driver).\n",
    "    :param source_table: Имя исходной таблицы (схема.таблица).\n",
    "    :param target_table: Имя целевой таблицы в DWH (схема.таблица).\n",
    "    :param selected_columns: Список колонок для выборки из исходной таблицы.\n",
    "    :param mode: Режим записи ('overwrite' или 'append'). По умолчанию 'append'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Чтение данных из исходной таблицы\n",
    "    df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=source_table,\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    # Выбор необходимых колонок\n",
    "    df_selected = df.select(*selected_columns)\n",
    "    \n",
    "    # Добавление колонки с временем загрузки\n",
    "    df_selected = df_selected.withColumn(\"load_dttm\", current_timestamp())\n",
    "    \n",
    "    # Запись данных в целевую таблицу\n",
    "    df_selected.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=target_table,\n",
    "        mode=mode,\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    print(f\"Таблица {target_table} успешно загружена.\")\n",
    "    return df_selected\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2478ab1f-65ae-4880-acef-5751a3ffd087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.d_craftsmans успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки dwh.d_craftsmans\n",
    "source_table_craftsmans = \"source3.craft_market_craftsmans\"\n",
    "target_table_craftsmans = \"dwh.d_craftsmans\"\n",
    "selected_columns_craftsmans = [\n",
    "    # \"craftsman_id\",\n",
    "    \"craftsman_name\",\n",
    "    \"craftsman_address\",\n",
    "    \"craftsman_birthday\",\n",
    "    \"craftsman_email\"\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.d_craftsmans\n",
    "dwh_d_craftsmans = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_craftsmans,\n",
    "    target_table=target_table_craftsmans,\n",
    "    selected_columns=selected_columns_craftsmans,\n",
    "    mode='append'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aed554e-28fa-4a7c-a68e-dcf83c9df713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.d_customers успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки dwh.d_customers\n",
    "source_table_customers = \"source3.craft_market_customers\"\n",
    "target_table_customers = \"dwh.d_customers\"\n",
    "selected_columns_customers = [\n",
    "    # \"customer_id\",\n",
    "    \"customer_name\",\n",
    "    \"customer_address\",\n",
    "    \"customer_birthday\",\n",
    "    \"customer_email\"\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.d_customers\n",
    "dwh_d_customers = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_customers,\n",
    "    target_table=target_table_customers,\n",
    "    selected_columns=selected_columns_customers,\n",
    "    mode='append'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d4504ac-b0c9-4c6a-9f26-f420cc1cc692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer_name='Anna-maria Lamba', customer_address='8 Prairieview Alley', customer_birthday=datetime.date(1990, 7, 27), customer_email='alamba1@ted.com', load_dttm=datetime.datetime(2024, 12, 26, 4, 5, 13, 700602)),\n",
       " Row(customer_name='Kim Coonihan', customer_address='3 Hanson Center', customer_birthday=datetime.date(1992, 5, 26), customer_email='kcoonihan2@angelfire.com', load_dttm=datetime.datetime(2024, 12, 26, 4, 5, 13, 700602)),\n",
       " Row(customer_name='Lanny Vasse', customer_address='9 Westridge Alley', customer_birthday=datetime.date(2003, 6, 12), customer_email='lvasse3@pbs.org', load_dttm=datetime.datetime(2024, 12, 26, 4, 5, 13, 700602)),\n",
       " Row(customer_name='Alice Treneman', customer_address='68462 Meadow Valley Drive', customer_birthday=datetime.date(1995, 7, 26), customer_email='atreneman4@cmu.edu', load_dttm=datetime.datetime(2024, 12, 26, 4, 5, 13, 700602)),\n",
       " Row(customer_name='Flo Morant', customer_address='2453 Crescent Oaks Avenue', customer_birthday=datetime.date(1997, 10, 31), customer_email='fmorant5@shinystat.com', load_dttm=datetime.datetime(2024, 12, 26, 4, 5, 13, 700602))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwh_d_customers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82eca057-6aef-4ce6-9504-f1e4eaa99f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.d_products успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки dwh.d_products\n",
    "source_table_products = \"source2.craft_market_masters_products\"  # Предполагается, что products находятся здесь\n",
    "target_table_products = \"dwh.d_products\"\n",
    "selected_columns_products = [\n",
    "    # \"product_id\",\n",
    "    \"product_name\",\n",
    "    \"product_description\",\n",
    "    \"product_type\",\n",
    "    \"product_price\"\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.d_products\n",
    "dwh_d_products = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_products,\n",
    "    target_table=target_table_products,\n",
    "    selected_columns=selected_columns_products,\n",
    "    mode='append'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf67021-4cc6-49ab-9a19-15e366330bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица dwh.f_orders успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "source_table_orders = \"source1.craft_market_wide\"\n",
    "target_table_orders = \"dwh.f_orders\"\n",
    "selected_columns_orders = [\n",
    "    \"order_created_date\",\n",
    "    \"order_completion_date\",\n",
    "    \"order_status\",\n",
    "    \"customer_id\",\n",
    "    \"craftsman_id\",\n",
    "    \"product_id\",\n",
    "    \"order_id\",\n",
    "]\n",
    "\n",
    "# Вызов функции для загрузки dwh.f_orders\n",
    "dwh_f_orders = load_table(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_orders,\n",
    "    target_table=target_table_orders,\n",
    "    selected_columns=selected_columns_orders,\n",
    "    mode='append'  # Используем 'append' для добавления данных\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e32350-20b1-4649-9dca-3aaad2a1c85d",
   "metadata": {},
   "source": [
    "# Инкрементальная Загрузка Витрины Данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78412350-ca78-41ee-9ecd-f354ffecde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, expr, count, floor\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def load_datamart_incremental(spark, jdbc_url, connection_properties, source_table, load_dates_table, target_datamart_table, report_period):\n",
    "    \"\"\"\n",
    "    Инкрементально загружает данные в витрину данных dwh.craftsman_report_datamart.\n",
    "    \n",
    "    :param spark: SparkSession\n",
    "    :param jdbc_url: JDBC URL для подключения к PostgreSQL.\n",
    "    :param connection_properties: Словарь с параметрами подключения (user, password, driver).\n",
    "    :param source_table: Таблица фактов (dwh.f_orders).\n",
    "    :param load_dates_table: Таблица для отслеживания дат загрузки (dwh.load_dates_craftsman_report_datamart).\n",
    "    :param target_datamart_table: Витрина данных (dwh.craftsman_report_datamart).\n",
    "    :param report_period: Отчетный период в формате 'YYYY-MM'.\n",
    "    \"\"\"\n",
    "    # Шаг 1: Получаем последнюю дату загрузки\n",
    "    last_load_date_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=load_dates_table,\n",
    "        properties=connection_properties\n",
    "    ).orderBy(col(\"load_dttm\").desc()).limit(1)\n",
    "    \n",
    "    if last_load_date_df.count() > 0:\n",
    "        last_load_date = last_load_date_df.collect()[0]['load_dttm']\n",
    "        print(f\"Последняя дата загрузки: {last_load_date}\")\n",
    "    else:\n",
    "        last_load_date = lit('1970-01-01').cast(TimestampType())  # Начальное значение\n",
    "        print(\"Таблица load_dates пуста. Загружаем все данные.\")\n",
    "    \n",
    "    # Шаг 2: Извлекаем новые или обновлённые записи из таблицы фактов\n",
    "    df_new_orders = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=source_table,\n",
    "        properties=connection_properties\n",
    "    ).filter(col(\"load_dttm\") > last_load_date)\n",
    "    \n",
    "    if df_new_orders.count() == 0:\n",
    "        print(\"Нет новых данных для загрузки.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Загружаем {df_new_orders.count()} новых записей из таблицы фактов.\")\n",
    "    \n",
    "    # Шаг 3: Объединяем данные с таблицами измерений\n",
    "    df_joined = df_new_orders \\\n",
    "        .join(\n",
    "            spark.read.jdbc(url=jdbc_url, table=\"dwh.d_craftsmans\", properties=connection_properties),\n",
    "            on=\"craftsman_id\",\n",
    "            how=\"left\"\n",
    "        ) \\\n",
    "        .join(\n",
    "            spark.read.jdbc(url=jdbc_url, table=\"dwh.d_customers\", properties=connection_properties),\n",
    "            on=\"customer_id\",\n",
    "            how=\"left\"\n",
    "        ) \\\n",
    "        .join(\n",
    "            spark.read.jdbc(url=jdbc_url, table=\"dwh.d_products\", properties=connection_properties),\n",
    "            on=\"product_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    # Шаг 4: Выполняем агрегации для расчёта метрик, исключая top_product_category\n",
    "    df_aggregated = df_joined.groupBy(\n",
    "        \"craftsman_id\", \n",
    "        \"craftsman_name\", \n",
    "        \"craftsman_address\", \n",
    "        \"craftsman_birthday\", \n",
    "        \"craftsman_email\"\n",
    "    ).agg(\n",
    "        (F.sum(\"product_price\") * 0.9).alias(\"craftsman_money\"),  # 10% платформа\n",
    "        (F.sum(\"product_price\") * 0.1).alias(\"platform_money\"),\n",
    "        F.count(\"order_id\").alias(\"count_order\"),\n",
    "        F.avg(\"product_price\").alias(\"avg_price_order\"),\n",
    "        F.avg(F.floor(F.months_between(F.current_date(), F.col(\"customer_birthday\")) / 12)).alias(\"avg_age_customer\"),\n",
    "        F.expr(\"percentile_approx(datediff(order_completion_date, order_created_date), 0.5)\").alias(\"median_time_order_completed\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"created\", 1)).alias(\"count_order_created\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"in progress\", 1)).alias(\"count_order_in_progress\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"delivery\", 1)).alias(\"count_order_delivery\"),\n",
    "        F.count(F.when(col(\"order_status\") == \"done\", 1)).alias(\"count_order_done\"),\n",
    "        F.count(F.when(~col(\"order_status\").isin(\"done\"), 1)).alias(\"count_order_not_done\")\n",
    "    )\n",
    "    \n",
    "    # Шаг 5: Вычисляем top_product_category отдельно\n",
    "    # Считаем количество каждого product_type для каждого craftsman_id\n",
    "    df_product_counts = df_joined.groupBy(\"craftsman_id\", \"product_type\") \\\n",
    "        .agg(F.count(\"*\").alias(\"product_type_count\"))\n",
    "    \n",
    "    # Определяем самое частое product_type для каждого craftsman_id\n",
    "    window_spec = Window.partitionBy(\"craftsman_id\").orderBy(F.desc(\"product_type_count\"))\n",
    "    \n",
    "    df_top_product = df_product_counts.withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "        .filter(F.col(\"rank\") == 1) \\\n",
    "        .select(\"craftsman_id\", F.col(\"product_type\").alias(\"top_product_category\"))\n",
    "    \n",
    "    # Шаг 6: Объединяем агрегированные данные с top_product_category\n",
    "    df_final = df_aggregated.join(df_top_product, on=\"craftsman_id\", how=\"left\") \\\n",
    "        .withColumn(\"report_period\", lit(report_period)) \\\n",
    "        # .withColumn(\"load_dttm\", F.current_timestamp())\n",
    "    return df_final\n",
    "    \n",
    "    # Шаг 7: Записываем агрегированные данные в витрину данных\n",
    "    df_final.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=target_datamart_table,\n",
    "        mode='append',\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    print(f\"Витрина данных {target_datamart_table} успешно обновлена.\")\n",
    "    \n",
    "    # Шаг 8: Обновляем таблицу дат загрузки\n",
    "    df_load_date = spark.range(1).withColumn(\"load_dttm\", current_timestamp()).select(\"load_dttm\")\n",
    "    df_load_date.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=load_dates_table,\n",
    "        mode='append',\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    \n",
    "    print(f\"Таблица {load_dates_table} обновлена.\")\n",
    "    return df_final, df_load_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abf42530-72fb-4a2c-b7b1-014dde6352c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица load_dates пуста. Загружаем все данные.\n",
      "Загружаем 999 новых записей из таблицы фактов.\n",
      "Витрина данных dwh.craftsman_report_datamart успешно обновлена.\n"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `load_dttm`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1630\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1670\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1671\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1672\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1673\u001b[0m     )\n\u001b[1;32m   1675\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `set`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1681\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1678\u001b[0m     fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1679\u001b[0m         StructField(\n\u001b[1;32m   1680\u001b[0m             k,\n\u001b[0;32m-> 1681\u001b[0m             \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m                \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1687\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1688\u001b[0m         )\n\u001b[1;32m   1689\u001b[0m     )\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1636\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1637\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSUPPORTED_DATA_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1638\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1639\u001b[0m     )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [UNSUPPORTED_DATA_TYPE] Unsupported DataType `set`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1630\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1691\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1691\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1692\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_TYPE_FOR_FIELD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1693\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: k},\n\u001b[1;32m   1694\u001b[0m         )\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `_field_names`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1681\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1678\u001b[0m     fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1679\u001b[0m         StructField(\n\u001b[1;32m   1680\u001b[0m             k,\n\u001b[0;32m-> 1681\u001b[0m             \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m                \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1687\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1688\u001b[0m         )\n\u001b[1;32m   1689\u001b[0m     )\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1636\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1637\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSUPPORTED_DATA_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1638\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1639\u001b[0m     )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [UNSUPPORTED_DATA_TYPE] Unsupported DataType `JavaObject`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1630\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1691\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1691\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1692\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_TYPE_FOR_FIELD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1693\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: k},\n\u001b[1;32m   1694\u001b[0m         )\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `_jc`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1681\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1678\u001b[0m     fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1679\u001b[0m         StructField(\n\u001b[1;32m   1680\u001b[0m             k,\n\u001b[0;32m-> 1681\u001b[0m             \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m                \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1687\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1688\u001b[0m         )\n\u001b[1;32m   1689\u001b[0m     )\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1636\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1637\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSUPPORTED_DATA_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1638\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1639\u001b[0m     )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [UNSUPPORTED_DATA_TYPE] Unsupported DataType `Column`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m report_period \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2021-03\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Формат: 'ГГГГ-ММ'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Вызов функции для инкрементальной загрузки витрины данных\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# dwh_load_dates_craftsman_report_datamart, dwh_craftsman_report_datamart = load_datamart_incremental(\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m dwh_load_dates_craftsman_report_datamart \u001b[38;5;241m=\u001b[39m \u001b[43mload_datamart_incremental\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnection_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_table_for_datamart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_dates_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_dates_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_datamart_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_datamart_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_period\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 114\u001b[0m, in \u001b[0;36mload_datamart_incremental\u001b[0;34m(spark, jdbc_url, connection_properties, source_table, load_dates_table, target_datamart_table, report_period)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Шаг 8: Обновляем таблицу дат загрузки\u001b[39;00m\n\u001b[1;32m    113\u001b[0m current_load_dttm \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcurrent_timestamp()\n\u001b[0;32m--> 114\u001b[0m df_load_date \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_load_dttm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_dttm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m df_load_date\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(\n\u001b[1;32m    116\u001b[0m     url\u001b[38;5;241m=\u001b[39mjdbc_url,\n\u001b[1;32m    117\u001b[0m     table\u001b[38;5;241m=\u001b[39mload_dates_table,\n\u001b[1;32m    118\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    119\u001b[0m     properties\u001b[38;5;241m=\u001b[39mconnection_properties\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mТаблица \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_dates_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m обновлена.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:955\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 955\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    972\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:958\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    956\u001b[0m     _merge_type,\n\u001b[1;32m    957\u001b[0m     (\n\u001b[0;32m--> 958\u001b[0m         \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[1;32m    966\u001b[0m     ),\n\u001b[1;32m    967\u001b[0m )\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    972\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1691\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1679\u001b[0m             StructField(\n\u001b[1;32m   1680\u001b[0m                 k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1688\u001b[0m             )\n\u001b[1;32m   1689\u001b[0m         )\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1691\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1692\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_TYPE_FOR_FIELD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1693\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: k},\n\u001b[1;32m   1694\u001b[0m         )\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `load_dttm`."
     ]
    }
   ],
   "source": [
    "# Параметры для загрузки витрины данных\n",
    "source_table_for_datamart = \"dwh.f_orders\"  # Источник данных для витрины\n",
    "load_dates_table = \"dwh.load_dates_craftsman_report_datamart\"\n",
    "target_datamart_table = \"dwh.craftsman_report_datamart\"\n",
    "report_period = \"2021-03\"  # Формат: 'ГГГГ-ММ'\n",
    "\n",
    "# Вызов функции для инкрементальной загрузки витрины данных\n",
    "dwh_load_dates_craftsman_report_datamart, dwh_craftsman_report_datamart = load_datamart_incremental(\n",
    "    spark=spark,\n",
    "    jdbc_url=jdbc_url,\n",
    "    connection_properties=connection_properties,\n",
    "    source_table=source_table_for_datamart,\n",
    "    load_dates_table=load_dates_table,\n",
    "    target_datamart_table=target_datamart_table,\n",
    "    report_period=report_period\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b98e2d-a366-4f46-abcb-8e9b56873975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
